# generate_rules_complete.py
import pandas as pd
from mlxtend.frequent_patterns import apriori, association_rules
import pickle
import os

# -----------------------------
# Charger dataset
# -----------------------------
dataset_path = os.path.join('..', 'data', 'meteorites_final_rebalanced.csv')
df = pd.read_csv(dataset_path)

print(f"Dataset charg√© : {len(df)} lignes")

# -----------------------------
# Colonnes √† utiliser pour apriori
# -----------------------------
columns = ["year_period", "mass_bin", "continent", "recclass_clean"]
df_small = df[columns].dropna()  # Supprimer les lignes avec valeurs manquantes

print(f"Donn√©es apr√®s nettoyage : {len(df_small)} lignes")
print(f"Valeurs uniques par colonne :")
for col in columns:
    print(f"  - {col}: {df_small[col].nunique()} valeurs")

# -----------------------------
# MODIFICATION 1: Encodage standard
# -----------------------------
df_encoded = pd.get_dummies(df_small).astype(bool)

print(f"Colonnes encod√©es : {len(df_encoded.columns)}")

# -----------------------------
# MODIFICATION 2: Support ULTRA-BAS pour MAX de r√®gles
# -----------------------------
# Support √† 0.0001 pour capturer TOUTES les r√®gles possibles
frequent_itemsets = apriori(df_encoded, min_support=0.0001, use_colnames=True)

print(f"Itemsets fr√©quents trouv√©s : {len(frequent_itemsets)}")

# -----------------------------
# MODIFICATION 3: G√©n√©ration avec CONFIDENCE comme m√©trique
# -----------------------------
# Utiliser confidence avec seuil √† 0.6 pour cibler directement
rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.6)

print(f"R√®gles g√©n√©r√©es (confidence ‚â• 0.6) : {len(rules)}")

# -----------------------------
# MODIFICATION 4: Filtrage LARGE pour garder TOUT
# -----------------------------
# Garder tout ce qui a confidence ‚â• 0.6 ET lift > 1.0
rules = rules[(rules['confidence'] >= 0.6) & (rules['lift'] > 1.0)]

print(f"R√®gles apr√®s filtrage basique : {len(rules)}")

# -----------------------------
# Filtrer pour garder les r√®gles qui pr√©disent un type
# -----------------------------
def has_type_in_consequents(row):
    return any('recclass_clean_' in str(item) for item in row['consequents'])

type_rules = rules[rules.apply(has_type_in_consequents, axis=1)]
other_rules = rules[~rules.apply(has_type_in_consequents, axis=1)]

print(f"R√®gles pr√©disant un type : {len(type_rules)}")
print(f"Autres r√®gles : {len(other_rules)}")

# -----------------------------
# MODIFICATION 5: √âquilibrer MAIS garder MAXIMUM
# -----------------------------
# Compter les r√®gles par type
type_to_rules = {}
for idx, row in type_rules.iterrows():
    for item in row['consequents']:
        if 'recclass_clean_' in str(item):
            type_name = str(item).replace('recclass_clean_', '')
            if type_name not in type_to_rules:
                type_to_rules[type_name] = []
            type_to_rules[type_name].append(idx)

print(f"\nTypes avec r√®gles : {len(type_to_rules)}")

# MAX_RULES_PER_TYPE TR√àS √âLEV√â
MAX_RULES_PER_TYPE = 500  # TR√àS HAUT pour garder maximum

balanced_indices = []
for type_name, indices in type_to_rules.items():
    if len(indices) <= MAX_RULES_PER_TYPE:
        # Type rare ‚Üí garder TOUTES ses r√®gles
        balanced_indices.extend(indices)
    else:
        # Type fr√©quent ‚Üí garder BEAUCOUP de r√®gles
        type_df = type_rules.loc[indices]
        
        # Trier par confidence (pour avoir moyenne √©lev√©e)
        type_df = type_df.sort_values('confidence', ascending=False)
        
        # Garder les MAX_RULES_PER_TYPE meilleures
        best_rules = type_df.head(MAX_RULES_PER_TYPE)
        balanced_indices.extend(best_rules.index.tolist())

balanced_type_rules = type_rules.loc[list(set(balanced_indices))]
print(f"R√®gles de type apr√®s √©quilibrage : {len(balanced_type_rules)}")

# Pour autres r√®gles, garder aussi BEAUCOUP
# Pas de filtrage suppl√©mentaire
print(f"Autres r√®gles conserv√©es : {len(other_rules)}")

# Concat√©ner TOUT
rules = pd.concat([balanced_type_rules, other_rules])

print(f"\nTotal r√®gles avant post-traitement : {len(rules)}")
# -----------------------------
# MODIFICATION 6: Post-traitement POUR AUGMENTER CONFIDENCE
# -----------------------------
# S√©parer les r√®gles par niveau de confidence
high_conf = rules[rules['confidence'] >= 0.7]  # Tr√®s hautes
medium_conf = rules[(rules['confidence'] >= 0.65) & (rules['confidence'] < 0.7)]  # Hautes
good_conf = rules[(rules['confidence'] >= 0.6) & (rules['confidence'] < 0.65)]  # Bonnes

print(f"\nüìä Distribution par confidence :")
print(f"  - ‚â• 0.7 : {len(high_conf)} r√®gles")
print(f"  - 0.65-0.7 : {len(medium_conf)} r√®gles")
print(f"  - 0.6-0.65 : {len(good_conf)} r√®gles")

# Strat√©gie: garder TOUTES les ‚â•0.7, 
# la plupart des 0.65-0.7 (surtout si bon lift),
# et certaines des 0.6-0.65 (uniquement si excellent lift)

# Pour medium_conf: garder si lift > 1.5
medium_conf = medium_conf[medium_conf['lift'] > 1.5]

# Pour good_conf: garder seulement si lift TR√àS bon (>2.0)
good_conf = good_conf[good_conf['lift'] > 2.0]

# Recombiner
rules = pd.concat([high_conf, medium_conf, good_conf])

print(f"\nR√®gles apr√®s optimisation confidence/lift : {len(rules)}")

# -----------------------------
# Ajouter colonnes utiles pour filtrage
# -----------------------------
rules['antecedents'] = rules['antecedents'].apply(lambda x: set(x))
rules['consequents'] = rules['consequents'].apply(lambda x: set(x))

# -----------------------------
# Statistiques finales D√âTAILL√âES
# -----------------------------
print("\n=== STATISTIQUES DES R√àGLES OPTIMIS√âES ===")
print(f"Total r√®gles : {len(rules)}")
print(f"Confidence moyenne : {rules['confidence'].mean():.3f}")
print(f"Lift moyen : {rules['lift'].mean():.3f}")
print(f"Support moyen : {rules['support'].mean():.6f}")

# Distribution d√©taill√©e confidence
print(f"\nüìä Distribution exacte confidence :")
for threshold in [0.6, 0.65, 0.7, 0.75, 0.8]:
    count = len(rules[rules['confidence'] >= threshold])
    percentage = (count / len(rules)) * 100 if len(rules) > 0 else 0
    print(f"  - ‚â• {threshold} : {count} r√®gles ({percentage:.1f}%)")

# Top types les plus pr√©dits
type_counts = {}
for _, row in balanced_type_rules.iterrows():
    for item in row['consequents']:
        if 'recclass_clean_' in str(item):
            type_name = str(item).replace('recclass_clean_', '')
            type_counts[type_name] = type_counts.get(type_name, 0) + 1

print("\nDistribution des r√®gles par type (top 10) :")
sorted_types = sorted(type_counts.items(), key=lambda x: -x[1])[:10]
for t, c in sorted_types:
    print(f"  - {t}: {c} r√®gles")

# -----------------------------
# V√©rifier les r√®gles pour les petites masses
# -----------------------------
small_mass_rules = rules[
    rules['antecedents'].apply(
        lambda x: any('<1g' in str(item) or '1-10g' in str(item) for item in x)
    )
]
print(f"\nR√®gles pour petites masses (<1g et 1-10g) : {len(small_mass_rules)}")

# -----------------------------
# TOP 10 r√®gles par confidence
# -----------------------------
print(f"\nüèÜ TOP 10 R√àGLES PAR CONFIDENCE :")
top_conf = rules.nlargest(10, 'confidence')
for idx, row in top_conf.iterrows():
    ants = ', '.join([str(a) for a in list(row['antecedents'])[:2]])
    cons = ', '.join([str(c) for c in list(row['consequents'])[:2]])
    print(f"\n  üîπ {ants} ‚Üí {cons}")
    print(f"     Confidence: {row['confidence']:.3f} | Lift: {row['lift']:.2f} | Support: {row['support']:.6f}")

# -----------------------------
# TOP 10 r√®gles par lift
# -----------------------------
print(f"\nüèÜ TOP 10 R√àGLES PAR LIFT :")
top_lift = rules.nlargest(10, 'lift')
for idx, row in top_lift.iterrows():
    ants = ', '.join([str(a) for a in list(row['antecedents'])[:2]])
    cons = ', '.join([str(c) for c in list(row['consequents'])[:2]])
    print(f"\n  üîπ {ants} ‚Üí {cons}")
    print(f"     Lift: {row['lift']:.2f} | Confidence: {row['confidence']:.3f} | Support: {row['support']:.6f}")

# -----------------------------
# Sauvegarder r√®gles
# -----------------------------
rules_path = os.path.join(os.path.dirname(__file__), 'rules.pkl')
with open(rules_path, 'wb') as f:
    pickle.dump(rules, f)
    print(f"\n" + "="*60)
print("‚úÖ Fichier rules.pkl cr√©√© avec succ√®s !")
print(f"   üìä TOTAL R√àGLES : {len(rules)}")
print(f"   ‚úÖ CONFIDENCE MOYENNE : {rules['confidence'].mean():.3f}")
print(f"   üéØ LIFT MOYEN : {rules['lift'].mean():.3f}")
print(f"   üìà Support moyen : {rules['support'].mean():.6f}")
print("="*60)